from typing import List, Dict, Any, Tuple, Optional, Set
import tiktoken
from functools import lru_cache
import asyncio
from rapidfuzz import fuzz
from datetime import datetime, timedelta
import re
import json
import numpy as np
from tenacity import retry, wait_exponential, stop_after_attempt

# -------------------------------------------------------------------
# 1. GeliÅŸmiÅŸ Token ve Bellek YÃ¶netimi
# -------------------------------------------------------------------

tokenizer = tiktoken.get_encoding("cl100k_base")

@lru_cache(maxsize=16_384)
def count_tokens(text: str) -> int:
    """LRU-cache ile aynÄ± stringler iÃ§in tekrar hesaplama yapmayÄ± Ã¶nler."""
    return len(tokenizer.encode(text))

class HistoryManager:
    def __init__(self, 
                 total_max_tokens: int = 131072, 
                 reserved_output_tokens: int = 65536,
                 client = None,
                 model_name: str = "Quik_v2:latest"):
        self.TOTAL_MAX_TOKENS = total_max_tokens
        self.RESERVED_OUTPUT_TOKENS = reserved_output_tokens
        self.AVAILABLE_INPUT_TOKENS = self.TOTAL_MAX_TOKENS - self.RESERVED_OUTPUT_TOKENS
        self.client = client  # Ollama client
        self.model_name = model_name
        self.conversation_state = {}  # Sohbet durumu metadata tutma
        self.key_concepts = set()  # Ã–nemli kavramlar iÃ§in set
        self.entity_memory = {}  # VarlÄ±k hafÄ±zasÄ±
        self.session_start_time = datetime.now()
        
    # -------------------------------------------------------------------
    # 2. AkÄ±llÄ± GeÃ§miÅŸ Trimming Sistemi
    # -------------------------------------------------------------------
    
    def trim_history_by_tokens(self, 
                             history: List[Dict[str, str]],
                             max_tokens: int = None,
                             decay_factor: float = 0.9) -> List[Dict[str, str]]:
        """
        GeliÅŸmiÅŸ history trimming:
        - Token bÃ¼tÃ§esine gÃ¶re adapt olur
        - Eski mesajlara gÃ¶re azalan bir Ã¶nem faktÃ¶rÃ¼ uygular
        - Ã–nemli mesajlarÄ± korur (etiketli ya da yÃ¼ksek iÃ§erik deÄŸeri)
        """
        if max_tokens is None:
            max_tokens = self.AVAILABLE_INPUT_TOKENS
            
        if not history:
            return []
            
        # Sohbet boyunca Ã¶nemli mesajlarÄ± belirle
        important_indices = self._identify_important_messages(history)
        
        # MesajlarÄ±n aÄŸÄ±rlÄ±klarÄ±nÄ± hesapla
        weights = []
        for i, msg in enumerate(history):
            # Baz aÄŸÄ±rlÄ±k
            weight = 1.0
            # Ã–nemli mesajsa daha yÃ¼ksek
            if i in important_indices:
                weight *= 2.0
            # Mesaj ne kadar eskiyse aÄŸÄ±rlÄ±ÄŸÄ± o kadar az - geometrik decay
            position_factor = decay_factor ** (len(history) - i - 1)
            weight *= position_factor
            weights.append(weight)
            
        # MesajlarÄ±n token uzunluklarÄ±nÄ± hesapla
        token_counts = []
        for msg in history:
            tk = msg.get('_tk')
            if tk is None:
                tk = count_tokens(msg.get("text", ""))
            msg['_tk'] = tk  # Cache'e al
            token_counts.append(tk)
            
        # Weighted knapsack problemi Ã§Ã¶zÃ¼mÃ¼
        kept_indices = self._weighted_knapsack(weights, token_counts, max_tokens)
        
        # Dizine gÃ¶re mesajlarÄ± al
        kept = [history[i] for i in sorted(kept_indices)]
        
        # Cache temizliÄŸi
        for msg in kept:
            msg.pop('_tk', None)
            
        return kept

    def _identify_important_messages(self, history: List[Dict[str, str]]) -> Set[int]:
        """Ã–nemli mesajlarÄ± tanÄ±mla."""
        important_indices = set()
        
        for i, msg in enumerate(history):
            text = msg.get("text", "").lower()
            
            # Sistem mesajlarÄ± Ã¶nemlidir
            if msg.get("role") == "system":
                important_indices.add(i)
                continue
                
            # Ã‡ok Ã¶nemli gÃ¶rÃ¼nen mesajlar
            if any(marker in text for marker in ["Ã¶nemli", "not", "hatÄ±rla", "unutma", "dikkat", "important", "note", "remember"]):
                important_indices.add(i)
                
            # URL iÃ§eren mesajlar
            if re.search(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', text):
                important_indices.add(i)
                
            # Soru iÃ§eren mesajlar
            if '?' in text:
                important_indices.add(i)
                
            # Ä°lk ve son mesajlar her zaman Ã¶nemli
            if i == 0 or i == len(history) - 1:
                important_indices.add(i)
                
        return important_indices
        
    def _weighted_knapsack(self, weights: List[float], costs: List[int], budget: int) -> Set[int]:
        """
        AÄŸÄ±rlÄ±klÄ± knapsack problem Ã§Ã¶zÃ¼mÃ¼:
        En deÄŸerli mesajlarÄ± seÃ§er ama bÃ¼tÃ§eyi aÅŸmaz
        """
        n = len(weights)
        # AÄŸÄ±rlÄ±k/maliyet oranÄ±na gÃ¶re sÄ±rala (verimlilik)
        items = [(i, weights[i]/costs[i]) for i in range(n) if costs[i] > 0]
        items.sort(key=lambda x: x[1], reverse=True)
        
        selected = set()
        total_cost = 0
        
        for idx, _ in items:
            if total_cost + costs[idx] <= budget:
                selected.add(idx)
                total_cost += costs[idx]
        
        return selected
    
    # -------------------------------------------------------------------
    # 3. Ä°Ã§erik Temelli Deduplication ve Optimizasyon
    # -------------------------------------------------------------------
    
    def deduplicate_history(self, 
                          history: List[Dict[str, str]], 
                          similarity_threshold: float = 85.0,
                          semantic_threshold: float = 80.0) -> List[Dict[str, str]]:
        """
        GeliÅŸmiÅŸ deduplication sistemi:
        - Hem metin benzerliÄŸi hem semantik benzerlik iÃ§in kontrol
        - Mesaj Ã§iftleri arasÄ± gecikmeli benzerlik (tekrarlama)
        - Ã–nemli ve son mesajlarÄ± koruma mantÄ±ÄŸÄ±
        """
        if len(history) <= 2:
            return history
            
        # Ã–nemli mesajlarÄ± belirle
        important_indices = self._identify_important_messages(history)
        
        deduped_history = []
        past_texts = []
        fingerprints = []  # Semantic fingerprints
        
        # Son beÅŸ mesajÄ± her zaman tut
        preserve_recent = min(5, len(history))
        recent_indices = set(range(len(history) - preserve_recent, len(history)))
        
        for i, msg in enumerate(history):
            text = msg.get("text", "").strip()
            role = msg.get("role", "user")
            
            if not text:
                continue
                
            # Her zaman korunmasÄ± gereken mesajlarÄ± ekle
            if i in important_indices or i in recent_indices:
                deduped_history.append(msg)
                past_texts.append(text)
                continue
                
            # YakÄ±n benzerlik kontrolÃ¼
            if self._is_similar_to_any(text, past_texts, similarity_threshold):
                continue
                
            deduped_history.append(msg)
            past_texts.append(text)
                
        return deduped_history
        
    def _is_similar_to_any(self, text: str, past_texts: List[str], threshold: float) -> bool:
        """Metnin geÃ§miÅŸ metinlere benzerliÄŸini kontrol eder."""
        for past in past_texts:
            # Token sort ratio - kelime sÄ±rasÄ± farklÄ± olsa bile benzerliÄŸi yakalar
            score = fuzz.token_sort_ratio(text, past)
            if score >= threshold:
                return True
                
            # Partial ratio - kÄ±smi eÅŸleÅŸmeleri yakalar
            score = fuzz.partial_ratio(text, past)
            if score >= threshold + 5:  # KÄ±smi eÅŸleÅŸmeler iÃ§in biraz daha yÃ¼ksek eÅŸik
                return True
                
        return False
        
    # -------------------------------------------------------------------
    # 4. Ã–zellik TabanlÄ± GeÃ§miÅŸ Ã–zeti
    # -------------------------------------------------------------------
    
    async def generate_enhanced_summary(self, 
                                      history: List[Dict[str, str]], 
                                      max_tokens: int = 700) -> Dict[str, Any]:
        """
        GeliÅŸmiÅŸ Ã¶zet oluÅŸturma: 
        - Sohbet iÃ§inde bulunan ana baÅŸlÄ±klarÄ±, varlÄ±klarÄ± ve Ã¶ÄŸeleri Ã§Ä±karÄ±r
        - Hem Ã¶zet hem de anahtar Ã¶ÄŸeler tablosu oluÅŸturur
        - Uzun dÃ¶nem hafÄ±za iÃ§in bilgi yapÄ±sÄ± hazÄ±rlar
        """
        if not history or len(history) < 8:  # KÃ¼Ã§Ã¼k sohbetler iÃ§in Ã¶zetleme yapmaz
            return {"summary": "", "entities": {}}
            
        # LLM iÃ§in input hazÄ±rla
        filtered = [msg for msg in history if msg["role"] in ("user", "assistant")]
        recent = filtered[-12:] if len(filtered) > 12 else filtered
        
        # Sohbet iÃ§eriÄŸini LLM iÃ§in formatla
        conversation_text = "\n".join(
            f"{msg['role'].capitalize()}: {msg['text'].strip()}" 
            for msg in recent if msg.get("text")
        )
        
        # Ã–zetleme iÃ§in geliÅŸmiÅŸ prompt
        summary_prompt = [
            {
                "role": "system",
                "content": (
                    "Kontekst analistliÄŸi gÃ¶revini Ã¼stleneceksin. AÅŸaÄŸÄ±daki konuÅŸmayÄ± inceleyerek ÅŸu Ã¼Ã§ ÅŸeyi yapacaksÄ±n:\n"
                    "1. Ã–ZET: KonuÅŸmanÄ±n ana konusunu ve Ã¶nemli noktalarÄ±nÄ± Ã¶zetleyen kÄ±sa bir paragraf (en fazla 200 kelime)\n"
                    "2. ANA KONULAR: KonuÅŸmada geÃ§en en Ã¶nemli 3-5 anahtar konu/fikir listesi\n"
                    "3. VARLIKLAR: KonuÅŸmada bahsedilen Ã¶nemli varlÄ±klar (kiÅŸiler, yerler, Ã¼rÃ¼nler, kavramlar vb) ve bunlarÄ±n tanÄ±mÄ±/aÃ§Ä±klamasÄ±\n\n"
                    "YanÄ±tÄ±nÄ± JSON formatÄ±nda dÃ¶ndÃ¼r. Gereksiz aÃ§Ä±klamalar yapma."
                )
            },
            {"role": "user", "content": conversation_text}
        ]
        
        try:
            # LLM'den Ã¶zet ve analiz iste
            response = await self._generate_with_retry(summary_prompt)
            
            # JSON Ã§Ä±ktÄ±sÄ±nÄ± parse etmeye Ã§alÄ±ÅŸ
            result = self._extract_json_from_text(response)
            if not result:
                # Fallback iÃ§in basit Ã¶zet dÃ¶ndÃ¼rme
                return {"summary": response[:500], "entities": {}}
                
            # Ã–nemli varlÄ±klarÄ± entity memory'ye ekle
            if "VARLIKLAR" in result:
                for entity, description in result["VARLIKLAR"].items():
                    self.entity_memory[entity] = {
                        "description": description,
                        "last_mentioned": datetime.now(),
                        "mention_count": self.entity_memory.get(entity, {}).get("mention_count", 0) + 1
                    }
                    
            # Ã–nemli kavramlarÄ± ekle
            if "ANA_KONULAR" in result:
                for topic in result["ANA_KONULAR"]:
                    self.key_concepts.add(topic)
                    
            return {
                "summary": result.get("Ã–ZET", ""),
                "topics": result.get("ANA_KONULAR", []),
                "entities": result.get("VARLIKLAR", {})
            }
            
        except Exception as e:
            print(f"Ã–zet oluÅŸturma hatasÄ±: {str(e)}")
            # Basit fallback Ã¶zet
            return {"summary": self._simple_summarize(history), "entities": {}}
            
    def _extract_json_from_text(self, text: str) -> Dict:
        """Metin iÃ§inden JSON yapÄ±yÄ± Ã§Ä±karma."""
        try:
            # Ã–nce tÃ¼m metni JSON olarak parse etmeyi dene
            return json.loads(text)
        except:
            # JSON blokunu regex ile bulmayÄ± dene
            json_pattern = r'```json\s*([\s\S]*?)\s*```'
            json_match = re.search(json_pattern, text)
            
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except:
                    pass
                    
            # Son Ã§are: Anahtar yapÄ±larÄ±n iÃ§eriÄŸini Ã§Ä±karmaya Ã§alÄ±ÅŸ
            result = {}
            
            # Ã–zet bÃ¶lÃ¼mÃ¼nÃ¼ bul
            summary_pattern = r'(?:Ã–ZET|Ã¶zet|Ã–zet):\s*(.*?)(?:\n\n|\n[A-Z]|$)'
            summary_match = re.search(summary_pattern, text, re.DOTALL)
            if summary_match:
                result["Ã–ZET"] = summary_match.group(1).strip()
                
            return result
            
    def _simple_summarize(self, history: List[Dict[str, str]], max_words: int = 200) -> str:
        """LLM Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z olduÄŸunda basit Ã¶zet Ã¼retme."""
        texts = [msg.get("text", "") for msg in history[-10:] if msg.get("role") == "user"]
        combined = " ".join(texts)
        words = combined.split()
        if len(words) <= max_words:
            return combined
        return " ".join(words[:max_words]) + "..."
        
    @retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
    async def _generate_with_retry(self, messages: List[Dict[str, str]]) -> str:
        """Retry mekanizmasÄ± ile LLM Ã§aÄŸrÄ±sÄ± yapma."""
        if not self.client:
            return ""
            
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=messages,
                stream=False,
                options={
                    "max_tokens": 1024,
                    "temperature": 0.3,
                    "top_p": 0.9,
                }
            )
            
            if "message" in response and "content" in response["message"]:
                return response["message"]["content"].strip()
            return ""
            
        except Exception as e:
            print(f"LLM Ã§aÄŸrÄ±sÄ± hatasÄ±: {str(e)}")
            raise
            
    # -------------------------------------------------------------------
    # 5. Ana EntryPoint - GeliÅŸmiÅŸ History Ä°ÅŸleme
    # -------------------------------------------------------------------
            
    async def process_history(self, 
                             history: List[Dict[str, str]], 
                             user_message: str,
                             force_summarize: bool = False) -> Tuple[List[Dict[str, str]], Dict[str, Any]]:
        """
        Ana history iÅŸleme fonksiyonu:
        - Token hesaplamasÄ± ve trimming
        - Deduplication
        - GerektiÄŸinde Ã¶zetleme
        - Bilgilerin durum bilgisi iÃ§in dÃ¶ndÃ¼rÃ¼lmesi
        """
        # Metadata dizileri
        context_metadata = {}
        
        # Token hesaplama
        user_message_tokens = count_tokens(user_message)
        available_history_tokens = self.AVAILABLE_INPUT_TOKENS - user_message_tokens
        
        # Ä°lk aÅŸama trimming
        trimmed_history = self.trim_history_by_tokens(
            history, 
            max_tokens=available_history_tokens,
            decay_factor=0.92  # Eski mesajlar iÃ§in decay faktÃ¶rÃ¼
        )
        
        # Duplicate temizliÄŸi
        deduped_history = self.deduplicate_history(
            trimmed_history,
            similarity_threshold=85.0
        )
        
        # Ã–zet gerekiyor mu kontrolÃ¼
        needs_summary = force_summarize or (len(deduped_history) > 15)
        summary_data = {"summary": "", "entities": {}, "topics": []}
        
        if needs_summary:
            summary_data = await self.generate_enhanced_summary(deduped_history)
            summary_text = summary_data.get("summary", "")
            
            if summary_text:
                # Ã–zeti sistem mesajÄ± olarak history baÅŸÄ±na ekle
                summary_msg = {
                    "role": "system",
                    "text": f"ğŸ“Œ KonuÅŸma Ã–zeti: {summary_text}"
                }
                
                # EÄŸer varlÄ±klar varsa, onlarÄ± da ekle
                entities = summary_data.get("entities", {})
                if entities:
                    entity_text = "\nğŸ“‹ Ã–nemli VarlÄ±klar: "
                    for entity, desc in entities.items():
                        if isinstance(desc, str):
                            entity_text += f"\n- {entity}: {desc}"
                        else:
                            entity_text += f"\n- {entity}"
                    
                    summary_msg["text"] += entity_text
                
                # Ã–zetleme sonrasÄ± durumu, kullanÄ±cÄ± mesajÄ± Ã¶nÃ¼ne ekle
                deduped_history.insert(0, summary_msg)
                
                # Son bir kez daha token bÃ¼tÃ§esine gÃ¶re kÄ±rpma
                deduped_history = self.trim_history_by_tokens(
                    deduped_history, 
                    max_tokens=available_history_tokens
                )
                
        # Metadata'yÄ± hazÄ±rla
        context_metadata = {
            "tokens": {
                "user_message": user_message_tokens,
                "history": sum(count_tokens(msg.get("text", "")) for msg in deduped_history),
                "available": available_history_tokens,
                "total": self.AVAILABLE_INPUT_TOKENS
            },
            "stats": {
                "original_message_count": len(history),
                "final_message_count": len(deduped_history),
                "compression_ratio": len(deduped_history) / max(1, len(history)),
                "summary_applied": needs_summary
            },
            "summary": summary_data
        }
            
        return deduped_history, context_metadata
        
    # -------------------------------------------------------------------
    # 6. Uzun DÃ¶nem HafÄ±za YÃ¶netimi (Long-Term Memory)
    # -------------------------------------------------------------------
    
    def update_entity_memory(self, message: Dict[str, str]) -> None:
        """Mesaj iÃ§eriÄŸini analiz ederek varlÄ±k hafÄ±zasÄ±nÄ± gÃ¼nceller."""
        text = message.get("text", "").lower()
        
        # Mevcut varlÄ±klarÄ± kontrol et
        for entity in list(self.entity_memory.keys()):  
            if entity.lower() in text:
                # VarlÄ±k mesajda geÃ§iyorsa gÃ¼ncelle
                self.entity_memory[entity]["last_mentioned"] = datetime.now()
                self.entity_memory[entity]["mention_count"] += 1
    
    def get_relevant_entities(self, query: str, max_entities: int = 3) -> Dict[str, Any]:
        """Sorgu ile en alakalÄ± varlÄ±klarÄ± dÃ¶ndÃ¼r."""
        if not self.entity_memory:
            return {}
            
        query = query.lower()
        scored_entities = []
        
        for entity, data in self.entity_memory.items():
            # Basit alaka skorlamasÄ±
            relevance = fuzz.partial_ratio(query, entity.lower()) / 100.0
            
            # Yenilik faktÃ¶rÃ¼ - yakÄ±n zamanda bahsedilen varlÄ±klar daha Ã¶nemli
            recency = 1.0
            if "last_mentioned" in data:
                time_diff = (datetime.now() - data["last_mentioned"]).total_seconds()
                # Son 5 dakika iÃ§inde bahsedildiyse maksimum yenilik
                recency = max(0.1, min(1.0, 300 / max(300, time_diff)))
                
            # PopÃ¼lerlik faktÃ¶rÃ¼ - Ã§ok bahsedilen varlÄ±klar daha Ã¶nemli
            popularity = min(1.0, data.get("mention_count", 0) / 5)
            
            # Toplam skor
            total_score = (relevance * 0.6) + (recency * 0.3) + (popularity * 0.1)
            
            scored_entities.append((entity, data, total_score))
            
        # Skora gÃ¶re sÄ±rala ve en Ã¼stteki varlÄ±klarÄ± dÃ¶ndÃ¼r
        scored_entities.sort(key=lambda x: x[2], reverse=True)
        
        result = {}
        for entity, data, score in scored_entities[:max_entities]:
            if score > 0.3:  # Minimum alaka eÅŸiÄŸi
                result[entity] = data
                
        return result